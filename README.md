# ğŸ“„ DocAI-RAG

A **production-style Document Question Answering system** built using **Retrieval-Augmented Generation (RAG)**. DocAI-RAG allows users to upload PDFs and ask natural language questions, with answers grounded strictly in the document content.

---

## âœ¨ Features

* ğŸ“¤ Upload and index PDF documents
* ğŸ” Semantic search using vector embeddings
* ğŸ§  Context-aware answers using LLMs (RAG)
* ğŸ“Š Confidence score for each answer
* ğŸ“„ Reference pages with similarity scores
* âš¡ FastAPI backend + Streamlit frontend
* ğŸ§± Modular, scalable architecture

---

## ğŸ§  Architecture Overview

```
User (Streamlit UI)
      â†“
FastAPI Backend
      â†“
PDF Ingestion Pipeline
      â†“
Text Chunking & Cleaning
      â†“
Vector Embeddings
      â†“
Vector Store (Chroma / FAISS)
      â†“
Retriever + LLM
      â†“
Answer + Confidence + Sources
```

---

## ğŸ› ï¸ Tech Stack

### Frontend

* Streamlit
* Custom CSS (Dark UI)

### Backend

* FastAPI
* Pydantic

### AI / ML

* Retrieval-Augmented Generation (RAG)
* Sentence Embeddings
* Similarity Search

### Vector Stores

* FAISS
* ChromaDB

### Utilities

* Python
* PDF parsing

---

## ğŸ“ Project Structure

```
Doc-AI/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI entry point
â”‚   â”œâ”€â”€ ingest.py            # PDF ingestion pipeline
â”‚   â”œâ”€â”€ qa_service.py        # Question answering logic
â”‚   â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ qa/
â”‚   â”œâ”€â”€ vector_store/
â”‚   â””â”€â”€ data/
â”‚
â”œâ”€â”€ app.py                   # Streamlit frontend
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸš€ How to Run Locally

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/your-username/DocAI-RAG.git
cd DocAI-RAG
```

### 2ï¸âƒ£ Create virtual environment

```bash
python -m venv venv
venv\Scripts\activate   # Windows
# source venv/bin/activate  # Mac/Linux
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Start backend

```bash
uvicorn backend.main:app --reload
```

### 5ï¸âƒ£ Start frontend

```bash
streamlit run app.py
```

---

## ğŸ§ª Example Workflow

1. Upload a PDF document
2. Backend indexes and stores embeddings
3. Ask a question in natural language
4. System retrieves relevant chunks
5. LLM generates grounded answer
6. UI displays answer, confidence, and references

---

## ğŸ“Œ Use Cases

* Academic notes Q&A
* Technical documentation assistant
* Company policy search
* Interview preparation from PDFs
* Knowledge base querying

---

## ğŸ”’ Answer Grounding

* Answers are **strictly generated from uploaded documents**
* Reference pages and similarity scores are shown
* Prevents hallucination by design

---

## ğŸ”® Future Improvements

* Multi-document support
* Chat history & memory
* User authentication
* Docker deployment
* Cloud vector DB support
* Better confidence calibration

---

## ğŸ‘¨â€ğŸ’» Author

**Sugan Raj**
Final Year CSE | AI & Backend Enthusiast

---

## â­ If you like this project

Give it a star â­ and feel free to fork or contribute!
